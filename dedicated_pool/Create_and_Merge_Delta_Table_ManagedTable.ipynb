{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spark Delta Table Merge Experiment. \n",
        "\n",
        "* This notebook will create a new spark database. Once the database is established, we can create multiple managed tables inside it and allow the data to be taken from the parquet files inside the data lake and push that into the delta table that is created using the merge command. Once the merge happens, the data in the delta table will have the most current version of the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(spark)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* First create the database for spark to store its tables. Once the database is created a warehouse directory gets created inside ADLS under the path datalake/synapse/workspaces/venkysyn1001/warehouse/spark_temperatures_db.db/\n",
        "\n",
        "<img src=\"./images/img_017.png\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "create database spark_temperatures_db;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "SHOW DATABASES;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "CREATE TABLE temperatures_ext_delta ( \n",
        "    latitude float, \n",
        "    longitude float, \n",
        "    time string,\n",
        "\ttemperature_2m float\n",
        ") USING DELTA;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "SELECT * from temperatures_ext_delta;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Even though the tables were created in the Synapse serverless pools, these tables are in ADLS and can be referenced directly inside the spark notebook giving the path to the ADLS directory. Note that the person running this notebook needs to have the permissions to the folder we are referencing to make this connection happen. This is very similar to how the pass thro auth happens in the serverless pools case. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "CREATE EXTERNAL TABLE temperatures_2018 \n",
        "USING PARQUET \n",
        "LOCATION \"abfss://datalake@venkydatalake1001.dfs.core.windows.net/temperatures/AirQualityIndexWithTemperatures_5/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Just count the records inside the external table we created. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "SELECT COUNT(*) FROM temperatures_2018"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Merge the dataset we have inside this external table, to the delta table we have created to be managed inside spark, and see how the data merges from the immutable parquet table to the managed spark table. Note the usage of the back tick symbol to escape the time that is a reserve word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "MERGE INTO temperatures_ext_delta AS TARGET\n",
        "USING temperatures_2018 AS SOURCE\n",
        "ON TARGET.latitude = SOURCE.latitude AND\n",
        "TARGET.longitude = SOURCE.longitude AND \n",
        "TARGET.`time` = SOURCE.`time` \n",
        "WHEN MATCHED THEN \n",
        "UPDATE SET \n",
        "    TARGET.temperature_2m = SOURCE.temperature_2m  \n",
        "WHEN NOT MATCHED THEN\n",
        "INSERT \n",
        "    ( TARGET.latitude, TARGET.longitude, TARGET.`time`, TARGET.temperature_2m )\n",
        "VALUES \n",
        "    ( SOURCE.latitude, SOURCE.longitude, SOURCE.`time`, SOURCE.temperature_2m )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* As we see the output that comes, the number of matches, and the rows that were inserted are shown. Assuming there were updates in the source data, the delta table will see those as updates, else it will see them as inserts. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "SELECT count(*) from temperatures_ext_delta;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Since we started out as an empty delta table, we can see that the number of records in the delta table equal the row counts from the 2018 data we just merged into it. \n",
        "\n",
        "<img src=\"./images/img_018.png\" />"
      ]
    }
  ],
  "metadata": {
    "description": "This will interact with the table we have created in the temperaturesdb and merge ",
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": false,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
