{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for the first time to setup.\n",
    "# !pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for the first time to setup.\n",
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* These 2 statements are required to ensure that the notebook finds the installation of spark on our machine and initializes the required paths/variables for spark to work locally. Local spark testing works pretty well for smaller datasets when the laptop is pretty well configured. I have 32GB of RAM and I can test even medium sized datasets pretty easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This snippet shows us how to run the spark pi program inside the notebook via VSCODE and see the output. This is a good sanity testing exercise to ensure we do not have any environment issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import random\n",
    "\n",
    "# This is already defined for us by synapse and the notebook will throw an error\n",
    "# if we try to create a new spark context. This is a change we have to make in the notebook \n",
    "# when we want to run inside synapse. \n",
    "\n",
    "# UNCOMMENT WHEN RUNNING LOCALLY!\n",
    "# sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "\n",
    "num_samples = 100000000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we can do some custom spark testing to ensure that we can do some computes passing some local datasets and getting somee basic aggregates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import min\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "TEMPERATURES_DS_PATH = \"file:///C:/Venky/AzureSynapseExperiments/datafiles/AirQualityIndexWithTemperatures_5/\"\n",
    "\n",
    "# This is the path that needs to be there inside Synapse\n",
    "TEMPERATURES_DS_PATH = \"abfss://files@venkydatalake1001.dfs.core.windows.net/temperatures/\"\n",
    "\n",
    "# The spark session create should be commented out when we are running inside Synapse \n",
    "# Otherwise it will throw a security violation error!\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Temperatures Analytics\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "print(\"Created spark session \" + str(spark))\n",
    "\n",
    "temperatures_ds = spark.read.parquet(TEMPERATURES_DS_PATH)\n",
    "temperatures_ds.show(10)\n",
    "\n",
    "print(\"Calculating min and max temperatures per lat lng\")\n",
    "temperatures_agg = temperatures_ds.groupBy(\"latitude\", \"longitude\").agg(\n",
    "    min(col(\"temperature_2m\")).alias(\"min_temp\"),\n",
    "    max(col(\"temperature_2m\")).alias(\"max_temp\")\n",
    ")\n",
    "\n",
    "temperatures_agg.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
