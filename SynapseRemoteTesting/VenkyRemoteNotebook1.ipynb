{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for the first time to setup.\n",
    "# pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for the first time to setup.\n",
    "# pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* These 2 statements are required to ensure that the notebook finds the installation of spark on our machine and initializes the required paths/variables for spark to work locally. Local spark testing works pretty well for smaller datasets when the laptop is pretty well configured. I have 32GB of RAM and I can test even medium sized datasets pretty easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This snippet shows us how to run the spark pi program inside the notebook via VSCODE and see the output. This is a good sanity testing exercise to ensure we do not have any environment issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import random\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we can do some custom spark testing to ensure that we can do some computes passing some local datasets and getting somee basic aggregates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created spark session <pyspark.sql.session.SparkSession object at 0x0000024D25F29C50>\n",
      "+---------+---------+----------------+--------------+\n",
      "| latitude|longitude|            time|temperature_2m|\n",
      "+---------+---------+----------------+--------------+\n",
      "|44.699997|44.199997|2018-01-01T00:00|           0.9|\n",
      "|44.699997|44.199997|2018-01-01T01:00|           0.9|\n",
      "|44.699997|44.199997|2018-01-01T02:00|           0.8|\n",
      "|44.699997|44.199997|2018-01-01T03:00|           0.8|\n",
      "|44.699997|44.199997|2018-01-01T04:00|           0.9|\n",
      "|44.699997|44.199997|2018-01-01T05:00|           1.3|\n",
      "|44.699997|44.199997|2018-01-01T06:00|           1.9|\n",
      "|44.699997|44.199997|2018-01-01T07:00|           2.7|\n",
      "|44.699997|44.199997|2018-01-01T08:00|           3.3|\n",
      "|44.699997|44.199997|2018-01-01T09:00|           4.1|\n",
      "+---------+---------+----------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Calculating min and max temperatures per lat lng\n",
      "+----------+-----------+-------------------+-------------------+\n",
      "|  latitude|  longitude|min(temperature_2m)|max(temperature_2m)|\n",
      "+----------+-----------+-------------------+-------------------+\n",
      "| 40.600006|      -79.7|              -25.8|               32.3|\n",
      "| 40.800003|      -73.5|              -18.0|               32.4|\n",
      "| 44.600006|      135.6|              -29.1|               31.6|\n",
      "|-18.400002| -49.199997|               12.6|               38.0|\n",
      "| 30.300003|  120.20001|              -11.2|               36.4|\n",
      "| 50.600006|   8.300003|              -11.3|               33.0|\n",
      "|  52.40001|   6.300003|               -8.3|               35.0|\n",
      "| 51.100006|       -3.0|               -5.3|               28.6|\n",
      "|      53.5| -2.0999908|               -5.9|               27.7|\n",
      "| 18.599998|   84.20001|               12.3|               37.1|\n",
      "| 32.300003|-107.799995|               -4.9|               40.1|\n",
      "| 4.0999985|      -74.9|               18.3|               36.7|\n",
      "| 16.200005|      -88.9|               16.6|               34.5|\n",
      "| 52.100006|       21.0|              -15.3|               30.7|\n",
      "|      56.5|  60.199997|              -26.7|               28.5|\n",
      "| 50.600006|        3.0|               -6.5|               35.4|\n",
      "|       8.0|   3.600006|               16.9|               37.5|\n",
      "| 44.100006|  43.100006|              -10.6|               34.0|\n",
      "|      18.0|      -95.5|               12.8|               38.3|\n",
      "|-11.299995|      -42.0|               15.4|               34.7|\n",
      "+----------+-----------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import min\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "TEMPERATURES_DS_PATH = \"file:///C:/Venky/AzureSynapseExperiments/datafiles/AirQualityIndexWithTemperatures_5/\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Temperatures Analytics\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "print(\"Created spark session \" + str(spark))\n",
    "\n",
    "temperatures_ds = spark.read.parquet(TEMPERATURES_DS_PATH)\n",
    "temperatures_ds.show(10)\n",
    "\n",
    "print(\"Calculating min and max temperatures per lat lng\")\n",
    "temperatures_agg = temperatures_ds.groupBy(\"latitude\", \"longitude\").agg(\n",
    "    min(col(\"temperature_2m\")).alias(\"min_temp\"),\n",
    "    max(col(\"temperature_2m\")).alias(\"max_temp\")\n",
    ")\n",
    "\n",
    "temperatures_agg.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
