# Spark on Kubernetes

* This experiment shows how we can run spark jobs on top of a kubernetes cluster. There are various steps involved in this process and we will walk through the steps to incrementally build a full fledged spark application.
* The first step in the process is to install docker desktop on top of our computer. Once the docker desktop is running, we will install kind and helm to get the basic spark-pi application to work on the kubernetes cluster using the spark operator. Refer to the steps <a href="InitialSetup.md">here</a>
* The next step in this process is to run a custom job coming from a docker image. We will create a new kind cluster with a custom yaml configuration to allow us to take a directory from the host machine and mount it on the kind docker container. Once that is done, we will build and push a custom docker image to the docker hub and use that to run the spark application. This application reads a json, prints it schema, converts it into a csv and writes it out to the external mount. Refer to the steps <a href="HostMounts.md">here</a>
